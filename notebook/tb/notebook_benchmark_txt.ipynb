{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 17:55:59.817377: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-11 17:55:59.838424: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 17:56:00.964047: I itex/core/wrapper/itex_gpu_wrapper.cc:35] Intel Extension for Tensorflow* GPU backend is loaded.\n",
      "2024-03-11 17:56:00.988030: I itex/core/wrapper/itex_cpu_wrapper.cc:52] Intel Extension for Tensorflow* AVX2 CPU backend is loaded.\n",
      "2024-03-11 17:56:01.027379: W itex/core/ops/op_init.cc:58] Op: _QuantizedMaxPool3D is already registered in Tensorflow\n",
      "2024-03-11 17:56:01.036112: E itex/core/devices/gpu/itex_gpu_runtime.cc:173] Can not found any devices. To check runtime environment on your host, please run itex/tools/env_check.sh.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n",
      "/home/thibaud/intel/oneapi/intelpython/envs/tensorflow/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "projectDir = '/home/thibaud/repos/anaconda/rakuten'\n",
    "if projectDir not in sys.path:\n",
    "    sys.path.append(projectDir)\n",
    "\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "#Define path in the config file\n",
    "import src.config as config\n",
    "config.path_to_project = projectDir #directory of the project\n",
    "config.path_to_data = os.path.join(projectDir, 'data', 'clean') #path to the data (dataframe)\n",
    "config.path_to_results = os.path.join(projectDir, 'results') #path to where the summary of the benchmark results will be saved (csv)\n",
    "config.path_to_images = os.path.join(projectDir, 'data', 'images', 'image_train_resized')  #path to the folder containing images\n",
    "config.path_to_models = os.path.join(projectDir, 'results') #path to the folder where the models will be saved\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.text.classifiers import TFbertClassifier\n",
    "from src.image.classifiers import ImgClassifier\n",
    "from src.multimodal.classifiers import TFmultiClassifier\n",
    "\n",
    "from src.utils.batch import fit_save_all\n",
    "from src.utils.plot import plot_training_history\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rakuten_img_path(img_folder, imageid, productid, suffix=''):\n",
    "    \"\"\" retrurns the path to the image of a given productid and imageid\"\"\"\n",
    "\n",
    "    df = pd.DataFrame(pd.concat([imageid, productid], axis=1))\n",
    "\n",
    "    img_path = df.apply(lambda row:\n",
    "                        os.path.join(img_folder, 'image_'\n",
    "                                     + str(row['imageid'])\n",
    "                                     + '_product_'\n",
    "                                     + str(row['productid'])\n",
    "                                     + suffix\n",
    "                                     + '.jpg'),\n",
    "                        axis=1)\n",
    "\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(os.path.join(config.path_to_data, 'df_train_index.csv'))\n",
    "data_train['testset'] = False\n",
    "data_test = pd.read_csv(os.path.join(config.path_to_data, 'df_test_index.csv'))\n",
    "data_test['testset'] = True\n",
    "data = pd.concat([data_train, data_test], axis=0)\n",
    "\n",
    "#merging text into token column\n",
    "colnames = ['designation_translated', 'description_translated'] #['designation', 'description']#\n",
    "data['tokens'] = data[colnames].apply(lambda row: ' '.join(s.lower() for s in row if isinstance(s, str)), axis=1)\n",
    "\n",
    "#path to images into img_path column\n",
    "data['img_path'] = Rakuten_img_path(img_folder=config.path_to_images,\n",
    "                             imageid=data['imageid'], productid=data['productid'], suffix='_resized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels of encoded classes\n",
    "class_labels = data.groupby('prdtypedesignation')['prdtypeindex'].first().reset_index()\n",
    "class_labels.index = class_labels['prdtypeindex']\n",
    "class_labels = class_labels.drop(columns='prdtypeindex').sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img_train = data.loc[~data['testset'], 'img_path']\n",
    "Img_test = data.loc[data['testset'], 'img_path']\n",
    "\n",
    "Txt_train = data.loc[~data['testset'], 'tokens']\n",
    "Txt_test = data.loc[data['testset'], 'tokens']\n",
    "\n",
    "y_train = data.loc[~data['testset'],'prdtypeindex']\n",
    "y_test = data.loc[data['testset'],'prdtypeindex']\n",
    "\n",
    "#To be fed into any of our sklearn classifiers, X_train and X_test\n",
    "#should be dataframes with columns tokens and img_path\n",
    "X_train = pd.DataFrame({'tokens': Txt_train, 'img_path': Img_train})\n",
    "X_test = pd.DataFrame({'tokens': Txt_test, 'img_path': Img_test})\n",
    "\n",
    "#All data for cross-validated scores\n",
    "X = pd.concat([X_train, X_test], axis=0)\n",
    "y = pd.concat([y_train, y_test], axis=0)\n",
    "\n",
    "#Number of classes\n",
    "num_classes = len(np.unique(data['prdtypeindex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLClassifier(base_name='LogisticRegression', vec_method='tfidf')\n",
    "# model = model.load('LogisticRegression_tfidf')\n",
    "# model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamètres : {'cbow__min_count': 3, 'cbow__vector_size': 100, 'cbow__window': 10, 'cbow__workers': 14, 'logreg__C': 10, 'logreg__max_iter': 2000}\n",
      "Meilleur score : 0.6853323912147441\n"
     ]
    }
   ],
   "source": [
    "import src.text.cbow_vectorizer as cbow\n",
    "import src.utils.plot as uplot\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "vcbow = cbow.CBowVectorizer()\n",
    "model = LogisticRegression()\n",
    "pipeline = Pipeline([\n",
    "    ('cbow', vcbow),\n",
    "    ('logreg', model)\n",
    "])\n",
    "# vector_size=500, window=10, min_count=2, workers=4\n",
    "param_grid = {\n",
    "    'cbow__vector_size': [100],\n",
    "    'cbow__window': [10],\n",
    "    'cbow__min_count': [2, 3, 4],\n",
    "    'cbow__workers': [14],\n",
    "    'logreg__C': [0.1, 1, 10],\n",
    "    'logreg__max_iter': [2000]\n",
    "}\n",
    "\n",
    "\n",
    "grid = HalvingGridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=14)\n",
    "\n",
    "grid.fit(X_train['tokens'], y_train)\n",
    "\n",
    "best_params = grid.best_params_\n",
    "best_score = grid.best_score_\n",
    "\n",
    "print(\"Meilleurs hyperparamètres :\", best_params)\n",
    "print(\"Meilleur score :\", best_score)\n",
    "\n",
    "\n",
    "\n",
    "# Meilleurs hyperparamètres : {'cbow__min_count': 2, 'cbow__vector_size': 300, 'cbow__window': 10, 'cbow__workers': 14, 'logreg__C': 0.1, 'logreg__max_iter': 2000}\n",
    "# Meilleur score : 0.6876582464817759\n",
    "\n",
    "# Meilleurs hyperparamètres : {'cbow__min_count': 3, 'cbow__vector_size': 100, 'cbow__window': 10, 'cbow__workers': 14, 'logreg__C': 10, 'logreg__max_iter': 2000}\n",
    "# Meilleur score : 0.6853323912147441\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "uplot.classification_results(y_test, y_pred, class_labels, 'LogisticRegression_cbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy classifier on tfidf\n",
    "# dum_classifier = MLClassifier(base_name='dummyclassifier')\n",
    "# dum_classifier.fit(X_train, y_train);\n",
    "# dum_classifier.classification_score(X_test, y_test)\n",
    "# cv_scores = dum_classifier.cross_validate(X, y, cv=10)\n",
    "# dum_classifier.save('text/dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of word based benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting:  LogisticRegression tfidf\n",
      "{'C': 0.1, 'penalty': 'l2', 'max_iter': 2000}\n",
      "GridSearch:  {'C': 0.1, 'max_iter': 2000, 'penalty': 'l2'}\n",
      "Test set, f1score:  0.6817626342598585\n"
     ]
    }
   ],
   "source": [
    "params_list = []\n",
    "class_type = 'MLClassifier'\n",
    "\n",
    "#grid search number of folds\n",
    "nfolds_grid = 0\n",
    "\n",
    "#cross-validation of f1-score\n",
    "nfolds_cv = 0\n",
    "\n",
    "#Bag of word parameters\n",
    "vec_method = 'tfidf'\n",
    "\n",
    "params_list.append({'modality': 'text',\n",
    "                    'class': class_type,\n",
    "                    'base_name': 'LogisticRegression', \n",
    "                    'vec_method': vec_method, \n",
    "                    'param_grid': {'C': [0.1, 1, 2], 'penalty': ['l2'], 'max_iter': [2000]},\n",
    "                    'nfolds_grid': 3, 'nfolds_cv': nfolds_cv\n",
    "                   })\n",
    "# params_list.append({'modality': 'text',\n",
    "#                     'class': class_type,\n",
    "#                     'base_name': 'MultinomialNB', \n",
    "#                     'vec_method': vec_method, \n",
    "#                     'param_grid': {'alpha': [0.02], 'fit_prior': [True]},\n",
    "#                     'nfolds_grid': nfolds_grid, 'nfolds_cv': nfolds_cv\n",
    "# #                     })\n",
    "# params_list.append({'modality': 'text',\n",
    "#                     'class': class_type,\n",
    "#                     'base_name': 'RandomForestClassifier', \n",
    "#                     'vec_method': vec_method, \n",
    "#                     'param_grid': {'n_estimators': [200], 'max_depth': [500,1000]},\n",
    "#                     'nfolds_grid': 0, 'nfolds_cv': 0\n",
    "#                     })\n",
    "# params_list.append({'modality': 'text',\n",
    "#                     'class': class_type,\n",
    "#                     'base_name': 'xgboost', \n",
    "#                     'vec_method': vec_method, \n",
    "#                     'param_grid': {'n_estimators': [200], 'objective': ['multi:softprob'], 'max_depth':[6], 'reg_alpha':[0]},\n",
    "#                     'nfolds_grid': nfolds_grid, 'nfolds_cv': nfolds_cv\n",
    "#                     })\n",
    "# params_list.append({'modality': 'text',\n",
    "#                     'class': class_type,\n",
    "#                     'base_name': 'LinearSVC', \n",
    "#                     'vec_method': vec_method, \n",
    "#                     'param_grid': {'C': np.arange(0.5, 1.5, 0.1), 'penalty': ['l2']},\n",
    "#                     'nfolds_grid': 5, 'nfolds_cv': 10\n",
    "#                     })\n",
    "\n",
    "results = fit_save_all(params_list, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, result_file_name = 'results_benchmark_text.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3min57 Halving\n",
    "# Fitting:  LogisticRegression tfidf\n",
    "# {'C': 0.1, 'penalty': 'l2', 'max_iter': 2000}\n",
    "# GridSearch:  {'C': 2, 'max_iter': 2000, 'penalty': 'l2'}\n",
    "# Test set, f1score:  0.6817626342598585"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting:  LogisticRegression tfidf\n",
    "{'C': 0.1, 'penalty': 'l2', 'max_iter': 2000}\n",
    "GridSearch:  {'C': 0.1, 'max_iter': 2000, 'penalty': 'l2'}\n",
    "Test set, f1score:  0.6817626342598585"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and check the saved result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modality</th>\n",
       "      <th>class</th>\n",
       "      <th>vectorization</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tested_params</th>\n",
       "      <th>best_params</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_cv_test</th>\n",
       "      <th>score_cv_train</th>\n",
       "      <th>fit_cv_time</th>\n",
       "      <th>model_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>MLClassifier</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>{'C': array([0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1...</td>\n",
       "      <td>{'C': 0.5, 'penalty': 'l2'}</td>\n",
       "      <td>0.82398</td>\n",
       "      <td>[0.82403265 0.82531629 0.82968601 0.82028095 0...</td>\n",
       "      <td>[0.96525381 0.96556959 0.96546323 0.96552612 0...</td>\n",
       "      <td>[8.3740375  8.30071878 8.0570724  8.07186079 8...</td>\n",
       "      <td>text/LinearSVC_tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  modality         class vectorization classifier  \\\n",
       "0     text  MLClassifier         tfidf  LinearSVC   \n",
       "\n",
       "                                       tested_params  \\\n",
       "0  {'C': array([0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1...   \n",
       "\n",
       "                   best_params  score_test  \\\n",
       "0  {'C': 0.5, 'penalty': 'l2'}     0.82398   \n",
       "\n",
       "                                       score_cv_test  \\\n",
       "0  [0.82403265 0.82531629 0.82968601 0.82028095 0...   \n",
       "\n",
       "                                      score_cv_train  \\\n",
       "0  [0.96525381 0.96556959 0.96546323 0.96552612 0...   \n",
       "\n",
       "                                         fit_cv_time            model_path  \n",
       "0  [8.3740375  8.30071878 8.0570724  8.07186079 8...  text/LinearSVC_tfidf  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(os.path.join(config.path_to_results,'results_benchmark_text.csv'), index_col=0)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec based benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = []\n",
    "class_type = 'MLClassifier'\n",
    "\n",
    "#grid search number of folds\n",
    "nfolds_grid = 0\n",
    "\n",
    "#cross-validation of f1-score\n",
    "nfolds_cv = 0\n",
    "\n",
    "#Word2vec parameters\n",
    "vec_method = 'skipgram'\n",
    "vector_size = 500\n",
    "\n",
    "#List of parameters to batch over\n",
    "params_list.append({'modality': 'text',\n",
    "                    'class': class_type, \n",
    "                    'base_name': 'LogisticRegression',\n",
    "                    'vec_method': vec_method,\n",
    "                    'param_grid': {'C': [10], 'penalty': 'l2',\n",
    "                                   'vec_params':[{'workers': num_cores-1, 'vector_size': vector_size}]},\n",
    "                    'nfolds_grid': nfolds_grid, 'nfolds_cv': nfolds_cv                    \n",
    "                    })\n",
    "params_list.append({'modality': 'text',\n",
    "                    'class': class_type,\n",
    "                    'base_name': 'RandomForestClassifier', \n",
    "                    'vec_method': vec_method, \n",
    "                    'param_grid': {'n_estimators': [200], 'max_depth': [500],\n",
    "                                   'vec_params':[{'workers': num_cores-1, 'vector_size': vector_size}]},\n",
    "                    'nfolds_grid': nfolds_grid, 'nfolds_cv': nfolds_cv\n",
    "                    })\n",
    "params_list.append({'modality': 'text',\n",
    "                    'class': class_type,\n",
    "                    'base_name': 'xgboost', \n",
    "                    'vec_method': vec_method, \n",
    "                    'param_grid': {'n_estimators': [200], 'objective': ['multi:softprob'], 'max_depth':[6], 'reg_alpha':[0],\n",
    "                                   'vec_params':[{'workers': num_cores-1, 'vector_size': vector_size}]},\n",
    "                    'nfolds_grid': nfolds_grid, 'nfolds_cv': nfolds_cv\n",
    "                    })\n",
    "params_list.append({'modality': 'text',\n",
    "                    'class': class_type, \n",
    "                    'base_name': 'SVC',\n",
    "                    'vec_method': vec_method,\n",
    "                    'param_grid': {'C': [10], 'kernel': ['rbf'],\n",
    "                                   'vec_params':[{'workers': num_cores-1, 'vector_size': vector_size}]},\n",
    "                    'nfolds_grid': nfolds_grid, 'nfolds_cv': nfolds_cv\n",
    "                    })\n",
    "\n",
    "results = fit_save_all(params_list, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, result_file_name = 'results_benchmark_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and check the saved result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch and check the saved result file\n",
    "results = pd.read_csv(os.path.join(config.path_to_results,'results_benchmark_text.csv'), index_col=0)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage of ML classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7695062608525401"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVC on tfidf\n",
    "svc_w2v_classifier = MLClassifier(base_name='SVC', C=10, kernel='rbf', vec_method = 'skipgram', vec_params={'workers': num_cores-1, 'vector_size': 512})\n",
    "svc_w2v_classifier.fit(X_train, y_train);\n",
    "svc_w2v_classifier.classification_score(X_test, y_test)\n",
    "svc_w2v_classifier.cross_validate(X, y, cv=10)\n",
    "svc_w2v_classifier.save('text/SVC_skipgram')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rakuten",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
